# SAM3 å®Œæ•´ä½¿ç”¨æ•™å­¸

## ğŸ“š ç›®éŒ„
- [å°ˆæ¡ˆç°¡ä»‹](#å°ˆæ¡ˆç°¡ä»‹)
- [ç’°å¢ƒå®‰è£](#ç’°å¢ƒå®‰è£)
- [ç²å–æ¨¡å‹æ¬Šé™](#ç²å–æ¨¡å‹æ¬Šé™)
- [åŸºç¤ä½¿ç”¨](#åŸºç¤ä½¿ç”¨)
- [é€²éšåŠŸèƒ½](#é€²éšåŠŸèƒ½)
- [ç¯„ä¾‹ç¨‹å¼ç¢¼](#ç¯„ä¾‹ç¨‹å¼ç¢¼)
- [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)

---

## ğŸ¯ å°ˆæ¡ˆç°¡ä»‹

**SAM 3 (Segment Anything Model 3)** æ˜¯ Meta AI é–‹ç™¼çš„çµ±ä¸€åˆ†å‰²åŸºç¤æ¨¡å‹ï¼Œæ”¯æ´ï¼š

### æ ¸å¿ƒç‰¹æ€§
- âœ… **é–‹æ”¾è©å½™åˆ†å‰²** - æ”¯æ´ 270K+ ç¨ç‰¹æ¦‚å¿µ
- ğŸ“¸ **åœ–åƒåˆ†å‰²** - æ–‡å­—/é»/æ¡†/é®ç½©æç¤º
- ğŸ¬ **è¦–é »è¿½è¹¤** - è·¨å¹€ç‰©é«”è¿½è¹¤
- ğŸ¤– **Agent æ•´åˆ** - èˆ‡ LLM å”åŒå·¥ä½œ
- ğŸ¯ **848M åƒæ•¸** - é”åˆ°äººé¡æ•ˆèƒ½çš„ 75-80%

### ä¸‰ç¨®ä¸»è¦æ¨¡å¼

1. **PCS (Promptable Concept Segmentation)** - æ¦‚å¿µåˆ†å‰²
   - ä½¿ç”¨æ–‡å­—æè¿°åˆ†å‰²æ‰€æœ‰åŒ¹é…çš„ç‰©é«”
   - ä¾‹å¦‚: "ç©¿ç´…è‰²è¡£æœçš„äºº"ã€"åœ“å½¢ç‰©é«”"

2. **PVS (Promptable Visual Segmentation)** - è¦–è¦ºåˆ†å‰²
   - é¡ä¼¼ SAM1/SAM2 çš„äº’å‹•å¼åˆ†å‰²
   - ä½¿ç”¨é»æ“Šã€æ¡†é¸ç²¾ç¢ºåˆ†å‰²å–®ä¸€ç‰©é«”

3. **Agent æ¨¡å¼** - æ™ºèƒ½åˆ†å‰²åŠ©æ‰‹
   - çµåˆ MLLM è™•ç†è¤‡é›œæŸ¥è©¢
   - è‡ªå‹•åˆ†è§£ä»»å‹™ä¸¦åŸ·è¡Œ

---

## ğŸ› ï¸ ç’°å¢ƒå®‰è£

### 1. ç³»çµ±éœ€æ±‚

```bash
# ç¡¬é«”éœ€æ±‚
- NVIDIA GPU (å»ºè­° 16GB+ VRAM)
- CUDA 12.6 æˆ–æ›´é«˜ç‰ˆæœ¬
- 16GB+ RAM

# è»Ÿé«”éœ€æ±‚
- Python 3.12+
- PyTorch 2.7+
- Linux/macOS (Windows éœ€è¦ WSL2)
```

### 2. å‰µå»º Conda ç’°å¢ƒ

```bash
# å‰µå»ºæ–°ç’°å¢ƒ
conda create -n sam3 python=3.12
conda activate sam3

# å®‰è£ PyTorch (CUDA 12.6)
pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
```

### 3. å®‰è£ SAM3

```bash
# æ–¹æ³• 1: å¾ GitHub å®‰è£ (æ¨è–¦ç”¨æ–¼ç ”ç©¶)
git clone https://github.com/facebookresearch/sam3.git
cd sam3
pip install -e .

# æ–¹æ³• 2: å¾ Hugging Face Transformers ä½¿ç”¨ (æ¨è–¦ç”¨æ–¼æ‡‰ç”¨)
pip install transformers accelerate
# ä¸éœ€è¦é¡å¤–å®‰è£,ç›´æ¥ä½¿ç”¨ transformers API
```

### 4. å®‰è£é¡å¤–ä¾è³´

```bash
# ç”¨æ–¼ Jupyter Notebook ç¤ºä¾‹
pip install -e ".[notebooks]"

# ç”¨æ–¼è¨“ç·´å’Œé–‹ç™¼
pip install -e ".[train,dev]"

# æˆ–å€‹åˆ¥å®‰è£å¸¸ç”¨å¥—ä»¶
pip install jupyter matplotlib opencv-python pillow requests
```

---

## ğŸ”‘ ç²å–æ¨¡å‹æ¬Šé™

### æ­¥é©Ÿ 1: ç”³è«‹è¨ªå•æ¬Šé™

1. è¨ªå• [Hugging Face SAM3 é é¢](https://huggingface.co/facebook/sam3)
2. é»æ“Š "Access repository"
3. å¡«å¯«ç”³è«‹è¡¨å–®ä¸¦æäº¤
4. ç­‰å¾… Meta åœ˜éšŠæ‰¹å‡†ï¼ˆé€šå¸¸å¹¾å°æ™‚å…§ï¼‰

### æ­¥é©Ÿ 2: è¨­ç½® Hugging Face Token

```bash
# åœ¨ Hugging Face ç¶²ç«™ç”Ÿæˆ access token
# å‰å¾€: https://huggingface.co/settings/tokens

# ç™»éŒ„åˆ° Hugging Face CLI
huggingface-cli login

# æˆ–ä½¿ç”¨ Python
from huggingface_hub import login
login(token="your_token_here")
```

### æ­¥é©Ÿ 3: é©—è­‰è¨ªå•

```python
# æ¸¬è©¦æ˜¯å¦èƒ½è¨ªå•æ¨¡å‹
from transformers import Sam3Model
model = Sam3Model.from_pretrained("facebook/sam3")
print("âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹!")
```

---

## ğŸš€ åŸºç¤ä½¿ç”¨

### æ–¹æ³• A: ä½¿ç”¨åŸç”Ÿ SAM3 API (æ¨è–¦ç”¨æ–¼åœ–åƒ)

#### åœ–åƒåˆ†å‰² - æ–‡å­—æç¤º

```python
import torch
from PIL import Image
from sam3.model_builder import build_sam3_image_model
from sam3.model.sam3_image_processor import Sam3Processor

# 1. è¼‰å…¥æ¨¡å‹
model = build_sam3_image_model()
processor = Sam3Processor(model)

# 2. è¼‰å…¥åœ–åƒ
image = Image.open("your_image.jpg")
inference_state = processor.set_image(image)

# 3. ä½¿ç”¨æ–‡å­—æç¤ºåˆ†å‰²
output = processor.set_text_prompt(
    state=inference_state, 
    prompt="a red car"
)

# 4. ç²å–çµæœ
masks = output["masks"]      # åˆ†å‰²é®ç½©
boxes = output["boxes"]      # é‚Šç•Œæ¡†
scores = output["scores"]    # ä¿¡å¿ƒåˆ†æ•¸

print(f"æ‰¾åˆ° {len(masks)} å€‹ç‰©é«”")
```

#### è¦–é »è¿½è¹¤ - æ–‡å­—æç¤º

```python
from sam3.model_builder import build_sam3_video_predictor

# 1. å»ºç«‹è¦–é »é æ¸¬å™¨
video_predictor = build_sam3_video_predictor()

# 2. é–‹å§‹æœƒè©±
video_path = "your_video.mp4"  # æˆ– JPEG è³‡æ–™å¤¾è·¯å¾‘
response = video_predictor.handle_request(
    request=dict(
        type="start_session",
        resource_path=video_path,
    )
)
session_id = response["session_id"]

# 3. æ·»åŠ æ–‡å­—æç¤º
response = video_predictor.handle_request(
    request=dict(
        type="add_prompt",
        session_id=session_id,
        frame_index=0,
        text="person wearing red shirt",
    )
)

# 4. åœ¨è¦–é »ä¸­å‚³æ’­åˆ†å‰²
for frame_output in video_predictor.handle_stream_request(
    request=dict(
        type="propagate_in_video",
        session_id=session_id,
        propagation_direction="both",
    )
):
    frame_idx = frame_output["frame_idx"]
    masks = frame_output["outputs"]["masks"]
    print(f"è™•ç†ç¬¬ {frame_idx} å¹€")

# 5. é—œé–‰æœƒè©±
video_predictor.handle_request(
    request=dict(
        type="close_session",
        session_id=session_id,
    )
)
```

### æ–¹æ³• B: ä½¿ç”¨ Hugging Face Transformers API (æ¨è–¦)

#### åœ–åƒåˆ†å‰² - æ–‡å­—æç¤º

```python
from transformers import Sam3Processor, Sam3Model
import torch
from PIL import Image
import requests

device = "cuda" if torch.cuda.is_available() else "cpu"

# 1. è¼‰å…¥æ¨¡å‹å’Œè™•ç†å™¨
model = Sam3Model.from_pretrained("facebook/sam3").to(device)
processor = Sam3Processor.from_pretrained("facebook/sam3")

# 2. è¼‰å…¥åœ–åƒ
image_url = "http://images.cocodataset.org/val2017/000000077595.jpg"
image = Image.open(requests.get(image_url, stream=True).raw).convert("RGB")

# 3. è™•ç†è¼¸å…¥
inputs = processor(images=image, text="ear", return_tensors="pt").to(device)

# 4. æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)

# 5. å¾Œè™•ç†
results = processor.post_process_instance_segmentation(
    outputs,
    threshold=0.5,
    mask_threshold=0.5,
    target_sizes=inputs.get("original_sizes").tolist()
)[0]

# 6. çµæœ
print(f"æ‰¾åˆ° {len(results['masks'])} å€‹ç‰©é«”")
print(f"é®ç½©å½¢ç‹€: {results['masks'].shape}")
print(f"é‚Šç•Œæ¡†: {results['boxes'].shape}")
print(f"ä¿¡å¿ƒåˆ†æ•¸: {results['scores']}")
```

#### è¦–è¦ºåŒ–çµæœ

```python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

def overlay_masks(image, masks, boxes=None):
    """åœ¨åœ–åƒä¸Šç–ŠåŠ åˆ†å‰²é®ç½©"""
    image = image.convert("RGBA")
    masks = 255 * masks.cpu().numpy().astype(np.uint8)
    
    n_masks = masks.shape[0]
    cmap = matplotlib.colormaps.get_cmap("rainbow").resampled(n_masks)
    colors = [tuple(int(c * 255) for c in cmap(i)[:3]) for i in range(n_masks)]

    for i, (mask, color) in enumerate(zip(masks, colors)):
        mask_img = Image.fromarray(mask)
        overlay = Image.new("RGBA", image.size, color + (0,))
        alpha = mask_img.point(lambda v: int(v * 0.5))
        overlay.putalpha(alpha)
        image = Image.alpha_composite(image, overlay)
    
    return image

# ä½¿ç”¨
result_image = overlay_masks(image, results["masks"])
result_image.show()
# æˆ–å„²å­˜
result_image.save("result.png")
```

---

## ğŸ¨ é€²éšåŠŸèƒ½

### 1. å¤šç¨®æç¤ºçµ„åˆ

#### æ–‡å­— + æ¡†æç¤º

```python
# ä½¿ç”¨æ–‡å­—æè¿° "handle",ä½†æ’é™¤çˆå­æŠŠæ‰‹
text = "handle"
oven_handle_box = [40, 183, 318, 204]  # [x1, y1, x2, y2]

inputs = processor(
    images=kitchen_image,
    text=text,
    input_boxes=[[oven_handle_box]],
    input_boxes_labels=[[0]],  # 0 = è² é¢æç¤º (æ’é™¤)
    return_tensors="pt"
).to(device)

with torch.no_grad():
    outputs = model(**inputs)

results = processor.post_process_instance_segmentation(
    outputs,
    threshold=0.5,
    mask_threshold=0.5,
    target_sizes=inputs.get("original_sizes").tolist()
)[0]
# çµæœ: æœƒåˆ†å‰²æ‰€æœ‰æŠŠæ‰‹,ä½†ä¸åŒ…æ‹¬çˆå­æŠŠæ‰‹
```

#### å¤šå€‹æ¡†æç¤º (æ­£è² ä¾‹)

```python
# ä½¿ç”¨å…©å€‹æ­£é¢æ¡†ä¾†å®šç¾©æ¦‚å¿µ
dial_box = [59, 144, 76, 163]
button_box = [87, 148, 104, 159]

inputs = processor(
    images=kitchen_image,
    input_boxes=[[dial_box, button_box]],
    input_boxes_labels=[[1, 1]],  # å…©å€‹éƒ½æ˜¯æ­£é¢
    return_tensors="pt"
).to(device)

with torch.no_grad():
    outputs = model(**inputs)

results = processor.post_process_instance_segmentation(
    outputs,
    threshold=0.5,
    mask_threshold=0.5,
    target_sizes=inputs.get("original_sizes").tolist()
)[0]
# çµæœ: åˆ†å‰²æ‰€æœ‰é¡ä¼¼æ—‹éˆ•å’ŒæŒ‰éˆ•çš„ç‰©é«”
```

### 2. æ‰¹æ¬¡æ¨ç†

```python
# è™•ç†å¤šå¼µåœ–åƒ,ä¸åŒæç¤º
images = [image1, image2, image3]
text_prompts = ["cat", "dog", "car"]

inputs = processor(
    images=images, 
    text=text_prompts, 
    return_tensors="pt"
).to(device)

with torch.no_grad():
    outputs = model(**inputs)

results = processor.post_process_instance_segmentation(
    outputs,
    threshold=0.5,
    mask_threshold=0.5,
    target_sizes=inputs.get("original_sizes").tolist()
)

for i, result in enumerate(results):
    print(f"åœ–åƒ {i}: æ‰¾åˆ° {len(result['masks'])} å€‹ç‰©é«”")
```

### 3. è¦–é »åˆ†å‰² (Transformers API)

```python
from transformers import Sam3VideoModel, Sam3VideoProcessor
from transformers.video_utils import load_video
from accelerate import Accelerator

device = Accelerator().device
model = Sam3VideoModel.from_pretrained("facebook/sam3").to(device, dtype=torch.bfloat16)
processor = Sam3VideoProcessor.from_pretrained("facebook/sam3")

# è¼‰å…¥è¦–é »
video_url = "https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4"
video_frames, _ = load_video(video_url)

# åˆå§‹åŒ–æœƒè©±
inference_session = processor.init_video_session(
    video=video_frames,
    inference_device=device,
    processing_device="cpu",
    video_storage_device="cpu",
    dtype=torch.bfloat16,
)

# æ·»åŠ æ–‡å­—æç¤º
text = "person"
inference_session = processor.add_text_prompt(
    inference_session=inference_session,
    text=text,
)

# è™•ç†æ‰€æœ‰å¹€
outputs_per_frame = {}
for model_outputs in model.propagate_in_video_iterator(
    inference_session=inference_session, 
    max_frame_num_to_track=50
):
    processed_outputs = processor.postprocess_outputs(
        inference_session, 
        model_outputs
    )
    outputs_per_frame[model_outputs.frame_idx] = processed_outputs
    print(f"è™•ç†ç¬¬ {model_outputs.frame_idx} å¹€")

print(f"âœ… å®Œæˆ! è™•ç†äº† {len(outputs_per_frame)} å¹€")
```

### 4. äº’å‹•å¼åˆ†å‰² (SAM3 Tracker)

```python
from transformers import Sam3TrackerProcessor, Sam3TrackerModel

model = Sam3TrackerModel.from_pretrained("facebook/sam3").to(device)
processor = Sam3TrackerProcessor.from_pretrained("facebook/sam3")

image = Image.open("truck.jpg").convert("RGB")

# å–®é»é»æ“Š
input_points = [[[[500, 375]]]]  # [batch, obj, points, coords]
input_labels = [[[1]]]  # 1 = æ­£é¢é»æ“Š

inputs = processor(
    images=image, 
    input_points=input_points, 
    input_labels=input_labels, 
    return_tensors="pt"
).to(device)

with torch.no_grad():
    outputs = model(**inputs)

masks = processor.post_process_masks(
    outputs.pred_masks.cpu(), 
    inputs["original_sizes"]
)[0]

print(f"ç”Ÿæˆ {masks.shape[1]} å€‹é®ç½©å€™é¸")
```

### 5. è‡ªå‹•é®ç½©ç”Ÿæˆ

```python
from transformers import pipeline

# ä½¿ç”¨ pipeline API
generator = pipeline("mask-generation", model="facebook/sam3", device=0)

image_url = "https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/truck.jpg"
outputs = generator(image_url, points_per_batch=64)

print(f"è‡ªå‹•ç”Ÿæˆ {len(outputs['masks'])} å€‹é®ç½©")
```

---

## ğŸ“ ç¯„ä¾‹ç¨‹å¼ç¢¼

### å®Œæ•´åœ–åƒåˆ†å‰²ç¯„ä¾‹

```python
#!/usr/bin/env python3
"""
SAM3 åœ–åƒåˆ†å‰²å®Œæ•´ç¯„ä¾‹
æ”¯æ´å¤šç¨®æç¤ºé¡å‹å’Œè¦–è¦ºåŒ–
"""

import torch
from transformers import Sam3Processor, Sam3Model
from PIL import Image, ImageDraw, ImageFont
import requests
import matplotlib.pyplot as plt
import numpy as np

def load_model(device="cuda"):
    """è¼‰å…¥ SAM3 æ¨¡å‹"""
    model = Sam3Model.from_pretrained("facebook/sam3").to(device)
    processor = Sam3Processor.from_pretrained("facebook/sam3")
    return model, processor

def segment_with_text(model, processor, image, text_prompt, device="cuda"):
    """ä½¿ç”¨æ–‡å­—æç¤ºåˆ†å‰²"""
    inputs = processor(images=image, text=text_prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    results = processor.post_process_instance_segmentation(
        outputs,
        threshold=0.5,
        mask_threshold=0.5,
        target_sizes=inputs.get("original_sizes").tolist()
    )[0]
    
    return results

def visualize_results(image, results, text_prompt):
    """è¦–è¦ºåŒ–åˆ†å‰²çµæœ"""
    fig, axes = plt.subplots(1, 2, figsize=(15, 7))
    
    # åŸå§‹åœ–åƒ
    axes[0].imshow(image)
    axes[0].set_title("åŸå§‹åœ–åƒ")
    axes[0].axis('off')
    
    # åˆ†å‰²çµæœ
    axes[1].imshow(image)
    
    # ç–ŠåŠ é®ç½©å’Œé‚Šç•Œæ¡†
    masks = results['masks'].cpu().numpy()
    boxes = results['boxes'].cpu().numpy()
    scores = results['scores'].cpu().numpy()
    
    colors = plt.cm.rainbow(np.linspace(0, 1, len(masks)))
    
    for i, (mask, box, score, color) in enumerate(zip(masks, boxes, scores, colors)):
        # é¡¯ç¤ºé®ç½©
        axes[1].imshow(mask, alpha=0.3, cmap='jet')
        
        # é¡¯ç¤ºé‚Šç•Œæ¡†
        x1, y1, x2, y2 = box
        rect = plt.Rectangle(
            (x1, y1), x2-x1, y2-y1,
            linewidth=2, edgecolor=color, facecolor='none'
        )
        axes[1].add_patch(rect)
        
        # é¡¯ç¤ºåˆ†æ•¸
        axes[1].text(
            x1, y1-5, f'{score:.2f}',
            color='white', fontsize=10,
            bbox=dict(boxstyle='round', facecolor=color, alpha=0.7)
        )
    
    axes[1].set_title(f'åˆ†å‰²çµæœ: "{text_prompt}" ({len(masks)} å€‹ç‰©é«”)')
    axes[1].axis('off')
    
    plt.tight_layout()
    plt.savefig(f'sam3_result_{text_prompt.replace(" ", "_")}.png', dpi=150, bbox_inches='tight')
    plt.show()

def main():
    """ä¸»å‡½æ•¸"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    # è¼‰å…¥æ¨¡å‹
    print("è¼‰å…¥ SAM3 æ¨¡å‹...")
    model, processor = load_model(device)
    
    # è¼‰å…¥åœ–åƒ
    print("è¼‰å…¥åœ–åƒ...")
    image_url = "http://images.cocodataset.org/val2017/000000077595.jpg"
    image = Image.open(requests.get(image_url, stream=True).raw).convert("RGB")
    
    # åˆ†å‰²ä¸åŒæ¦‚å¿µ
    prompts = ["ear", "nose", "eye"]
    
    for prompt in prompts:
        print(f"\nåˆ†å‰²: {prompt}")
        results = segment_with_text(model, processor, image, prompt, device)
        print(f"æ‰¾åˆ° {len(results['masks'])} å€‹ç‰©é«”")
        visualize_results(image, results, prompt)

if __name__ == "__main__":
    main()
```

### è¦–é »è¿½è¹¤ç¯„ä¾‹

```python
#!/usr/bin/env python3
"""
SAM3 è¦–é »è¿½è¹¤ç¯„ä¾‹
è¿½è¹¤è¦–é »ä¸­çš„ç‰¹å®šç‰©é«”
"""

import torch
from transformers import Sam3VideoModel, Sam3VideoProcessor
from transformers.video_utils import load_video
from accelerate import Accelerator
import cv2
import numpy as np
from pathlib import Path

def track_objects_in_video(video_path, text_prompt, output_dir="output"):
    """åœ¨è¦–é »ä¸­è¿½è¹¤ç‰©é«”"""
    device = Accelerator().device
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    # è¼‰å…¥æ¨¡å‹
    print("è¼‰å…¥æ¨¡å‹...")
    model = Sam3VideoModel.from_pretrained("facebook/sam3").to(device, dtype=torch.bfloat16)
    processor = Sam3VideoProcessor.from_pretrained("facebook/sam3")
    
    # è¼‰å…¥è¦–é »
    print(f"è¼‰å…¥è¦–é »: {video_path}")
    video_frames, _ = load_video(video_path)
    print(f"è¦–é »å¹€æ•¸: {len(video_frames)}")
    
    # åˆå§‹åŒ–æœƒè©±
    print("åˆå§‹åŒ–æ¨ç†æœƒè©±...")
    inference_session = processor.init_video_session(
        video=video_frames,
        inference_device=device,
        processing_device="cpu",
        video_storage_device="cpu",
        dtype=torch.bfloat16,
    )
    
    # æ·»åŠ æ–‡å­—æç¤º
    print(f"æ·»åŠ æ–‡å­—æç¤º: '{text_prompt}'")
    inference_session = processor.add_text_prompt(
        inference_session=inference_session,
        text=text_prompt,
    )
    
    # è™•ç†è¦–é »
    print("é–‹å§‹è¿½è¹¤...")
    outputs_per_frame = {}
    
    for model_outputs in model.propagate_in_video_iterator(
        inference_session=inference_session
    ):
        processed_outputs = processor.postprocess_outputs(
            inference_session, 
            model_outputs
        )
        outputs_per_frame[model_outputs.frame_idx] = processed_outputs
        
        if (model_outputs.frame_idx + 1) % 30 == 0:
            print(f"å·²è™•ç† {model_outputs.frame_idx + 1} å¹€...")
    
    print(f"âœ… å®Œæˆ! è™•ç†äº† {len(outputs_per_frame)} å¹€")
    
    # ä¿å­˜çµæœ
    save_video_with_masks(video_frames, outputs_per_frame, output_dir, text_prompt)
    
    return outputs_per_frame

def save_video_with_masks(video_frames, outputs_per_frame, output_dir, text_prompt):
    """å°‡å¸¶æœ‰é®ç½©çš„è¦–é »ä¿å­˜"""
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    
    output_path = output_dir / f"tracked_{text_prompt.replace(' ', '_')}.mp4"
    
    # ç²å–è¦–é »å°ºå¯¸
    height, width = video_frames[0].shape[:2]
    
    # å‰µå»ºè¦–é »å¯«å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(str(output_path), fourcc, 30.0, (width, height))
    
    print(f"ä¿å­˜è¦–é »åˆ°: {output_path}")
    
    for frame_idx, frame in enumerate(video_frames):
        if frame_idx in outputs_per_frame:
            outputs = outputs_per_frame[frame_idx]
            
            # è½‰æ›ç‚º BGR
            frame_bgr = cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR)
            
            # ç–ŠåŠ é®ç½©
            if len(outputs['masks']) > 0:
                masks = outputs['masks'].cpu().numpy()
                boxes = outputs['boxes'].cpu().numpy()
                
                for mask, box in zip(masks, boxes):
                    # å‰µå»ºå½©è‰²é®ç½©
                    color = (0, 255, 0)  # ç¶ è‰²
                    colored_mask = np.zeros_like(frame_bgr)
                    colored_mask[mask > 0.5] = color
                    
                    # ç–ŠåŠ 
                    frame_bgr = cv2.addWeighted(frame_bgr, 1, colored_mask, 0.3, 0)
                    
                    # ç¹ªè£½é‚Šç•Œæ¡†
                    x1, y1, x2, y2 = box.astype(int)
                    cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)
            
            out.write(frame_bgr)
        else:
            out.write(cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR))
    
    out.release()
    print(f"âœ… è¦–é »å·²ä¿å­˜")

def main():
    """ä¸»å‡½æ•¸"""
    video_url = "https://huggingface.co/datasets/hf-internal-testing/sam2-fixtures/resolve/main/bedroom.mp4"
    text_prompt = "person"
    
    outputs = track_objects_in_video(video_url, text_prompt)
    
    # çµ±è¨ˆä¿¡æ¯
    total_detections = sum(len(out['object_ids']) for out in outputs.values())
    print(f"\nçµ±è¨ˆ:")
    print(f"  ç¸½å¹€æ•¸: {len(outputs)}")
    print(f"  ç¸½æª¢æ¸¬æ•¸: {total_detections}")
    print(f"  å¹³å‡æ¯å¹€: {total_detections / len(outputs):.1f} å€‹ç‰©é«”")

if __name__ == "__main__":
    main()
```

---

## â“ å¸¸è¦‹å•é¡Œ

### Q1: å¦‚ä½•è™•ç† CUDA out of memory éŒ¯èª¤?

```python
# è§£æ±ºæ–¹æ¡ˆ 1: é™ä½æ‰¹æ¬¡å¤§å°
inputs = processor(images=image, text=prompt, return_tensors="pt")
# ä¸€æ¬¡è™•ç†ä¸€å¼µ

# è§£æ±ºæ–¹æ¡ˆ 2: ä½¿ç”¨è¼ƒå°çš„åœ–åƒ
image = image.resize((800, 600))

# è§£æ±ºæ–¹æ¡ˆ 3: ä½¿ç”¨ bfloat16
model = model.to(dtype=torch.bfloat16)

# è§£æ±ºæ–¹æ¡ˆ 4: å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é» (è¨“ç·´æ™‚)
model.gradient_checkpointing_enable()
```

### Q2: å¦‚ä½•æé«˜åˆ†å‰²æº–ç¢ºåº¦?

```python
# æ–¹æ³• 1: èª¿æ•´é–¾å€¼
results = processor.post_process_instance_segmentation(
    outputs,
    threshold=0.3,  # é™ä½é–¾å€¼ç²å¾—æ›´å¤šæª¢æ¸¬
    mask_threshold=0.5,
    target_sizes=inputs.get("original_sizes").tolist()
)[0]

# æ–¹æ³• 2: ä½¿ç”¨æ›´å…·é«”çš„æ–‡å­—æç¤º
# âŒ ä¸å¥½: "object"
# âœ… å¥½: "red sports car with white stripes"

# æ–¹æ³• 3: çµ„åˆå¤šç¨®æç¤º
inputs = processor(
    images=image,
    text="car",
    input_boxes=[[approximate_box]],  # æ·»åŠ å¤§è‡´ä½ç½®
    input_boxes_labels=[[1]],
    return_tensors="pt"
)
```

### Q3: å¦‚ä½•è™•ç†å¤§å‹è¦–é »?

```python
# ä½¿ç”¨ä¸²æµæ¨¡å¼
inference_session = processor.init_video_session(
    inference_device=device,
    processing_device="cpu",  # CPU è™•ç†ä»¥ç¯€çœ GPU è¨˜æ†¶é«”
    video_storage_device="cpu",  # å°‡å¹€å­˜åœ¨ CPU
    dtype=torch.bfloat16,
)

# é€å¹€è™•ç†
for frame_idx, frame in enumerate(video_frames):
    inputs = processor(images=frame, device=device, return_tensors="pt")
    
    model_outputs = model(
        inference_session=inference_session,
        frame=inputs.pixel_values[0],
        reverse=False,
    )
    
    # ç«‹å³è™•ç†ä¸¦é‡‹æ”¾è¨˜æ†¶é«”
    processed = processor.postprocess_outputs(
        inference_session,
        model_outputs,
        original_sizes=inputs.original_sizes,
    )
    
    # ä¿å­˜æˆ–è™•ç†çµæœ
    save_frame_result(processed)
    
    # æ¸…ç†
    del inputs, model_outputs, processed
    torch.cuda.empty_cache()
```

### Q4: æ”¯æ´å“ªäº›èªè¨€çš„æ–‡å­—æç¤º?

SAM3 ä¸»è¦åœ¨è‹±æ–‡æ•¸æ“šä¸Šè¨“ç·´,ä½†ä¹Ÿæ”¯æ´å…¶ä»–èªè¨€:

```python
# è‹±æ–‡ (æœ€ä½³)
text = "a red car"

# ä¸­æ–‡ (éƒ¨åˆ†æ”¯æ´)
text = "ä¸€è¼›ç´…è‰²çš„æ±½è»Š"

# å…¶ä»–èªè¨€
text = "ein rotes Auto"  # å¾·æ–‡

# å»ºè­°: ä½¿ç”¨ç°¡å–®ã€æè¿°æ€§çš„è‹±æ–‡ä»¥ç²å¾—æœ€ä½³çµæœ
```

### Q5: å¦‚ä½•åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­éƒ¨ç½²?

```python
# å„ªåŒ–å»ºè­°

# 1. ä½¿ç”¨ TorchScript
model = torch.jit.script(model)

# 2. ä½¿ç”¨ ONNX (å¦‚æœæ”¯æ´)
torch.onnx.export(model, ...)

# 3. ä½¿ç”¨ FastAPI å»ºç«‹ API
from fastapi import FastAPI, File, UploadFile
import uvicorn

app = FastAPI()

@app.post("/segment/")
async def segment_image(file: UploadFile, text_prompt: str):
    image = Image.open(file.file)
    results = segment_with_text(model, processor, image, text_prompt)
    return {"masks": results["masks"].tolist(), "boxes": results["boxes"].tolist()}

# 4. ä½¿ç”¨ Docker å®¹å™¨åŒ–
# Dockerfile ç¯„ä¾‹
"""
FROM nvidia/cuda:12.6.0-runtime-ubuntu22.04
RUN pip install torch transformers
COPY . /app
CMD ["python", "server.py"]
"""

# 5. æ‰¹æ¬¡è™•ç†å„ªåŒ–
def batch_inference(images, prompts, batch_size=4):
    results = []
    for i in range(0, len(images), batch_size):
        batch_images = images[i:i+batch_size]
        batch_prompts = prompts[i:i+batch_size]
        batch_results = model_inference(batch_images, batch_prompts)
        results.extend(batch_results)
    return results
```

### Q6: å¦‚ä½•å¾®èª¿ SAM3 åœ¨è‡ªè¨‚æ•¸æ“šä¸Š?

```bash
# åƒè€ƒè¨“ç·´æ–‡æª”
cd sam3
pip install -e ".[train]"

# ä½¿ç”¨æä¾›çš„é…ç½®æ–‡ä»¶
python sam3/train/train.py -c configs/your_config.yaml

# è‡ªè¨‚æ•¸æ“šé›†æ ¼å¼ (COCO æ ¼å¼)
"""
{
  "images": [...],
  "annotations": [
    {
      "id": 1,
      "image_id": 1,
      "category_id": 1,
      "segmentation": [...],
      "bbox": [x, y, w, h],
      "area": ...,
      "iscrowd": 0
    }
  ],
  "categories": [...]
}
"""
```

---

## ğŸ“š æ›´å¤šè³‡æº

### å®˜æ–¹è³‡æº
- ğŸ“„ [è«–æ–‡](https://arxiv.org/abs/2511.16719)
- ğŸŒ [å°ˆæ¡ˆé é¢](https://ai.meta.com/sam3)
- ğŸ’» [GitHub Repo](https://github.com/facebookresearch/sam3)
- ğŸ¤— [Hugging Face Model](https://huggingface.co/facebook/sam3)
- ğŸ“ [Blog æ–‡ç« ](https://ai.meta.com/blog/segment-anything-model-3/)

### Jupyter Notebook ç¯„ä¾‹
- `sam3_image_predictor_example.ipynb` - åœ–åƒåˆ†å‰²
- `sam3_video_predictor_example.ipynb` - è¦–é »è¿½è¹¤
- `sam3_image_batched_inference.ipynb` - æ‰¹æ¬¡æ¨ç†
- `sam3_agent.ipynb` - Agent æ¨¡å¼
- `sam3_for_sam1_task_example.ipynb` - SAM1 ä»»å‹™
- `sam3_for_sam2_video_task_example.ipynb` - SAM2 ä»»å‹™

### ç¤¾ç¾¤è³‡æº
- ğŸ¨ [Hugging Face Spaces](https://huggingface.co/spaces/akhaliq/sam3) - ç·šä¸Š Demo
- ğŸ [Python ç¯„ä¾‹](https://github.com/facebookresearch/sam3/tree/main/examples)
- ğŸ“Š [è©•ä¼°è…³æœ¬](https://github.com/facebookresearch/sam3/tree/main/scripts/eval)

---

## ğŸ¯ å¿«é€Ÿé–‹å§‹æª¢æŸ¥æ¸…å–®

- [ ] âœ… å®‰è£ Python 3.12+ å’Œ PyTorch 2.7+
- [ ] âœ… å¾ GitHub æˆ– pip å®‰è£ SAM3
- [ ] âœ… ç”³è«‹ä¸¦ç²å¾— Hugging Face æ¨¡å‹è¨ªå•æ¬Šé™
- [ ] âœ… è¨­ç½® Hugging Face Token
- [ ] âœ… é‹è¡Œç¬¬ä¸€å€‹åœ–åƒåˆ†å‰²ç¯„ä¾‹
- [ ] âœ… å˜—è©¦ä¸åŒçš„æç¤ºé¡å‹
- [ ] âœ… æ¢ç´¢è¦–é »è¿½è¹¤åŠŸèƒ½
- [ ] âœ… æŸ¥çœ‹ Jupyter Notebook ç¯„ä¾‹

---

## ğŸ“§ æ”¯æ´

å¦‚æœ‰å•é¡Œ:
1. æŸ¥çœ‹ [GitHub Issues](https://github.com/facebookresearch/sam3/issues)
2. åƒè€ƒ [Hugging Face Discussions](https://huggingface.co/facebook/sam3/discussions)
3. é–±è®€å®˜æ–¹æ–‡æª”å’Œç¯„ä¾‹

---

**ç¥ä½ ä½¿ç”¨ SAM3 æ„‰å¿«! ğŸ‰**

æœ€å¾Œæ›´æ–°: 2025-11-25
