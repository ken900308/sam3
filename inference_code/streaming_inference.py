#!/usr/bin/env python3
"""
SAM3 å³æ™‚ä¸²æµæ¨è«–è…³æœ¬ (Webcam / è¦–è¨Šæª”æ¡ˆ / RTSP)
================================================
åŸºæ–¼å®˜æ–¹ examples ä¿®æ”¹ï¼Œæ”¯æ´å³æ™‚æ”å½±æ©Ÿä¸²æµ detection

ä½¿ç”¨æ–¹æ³•:
    python streaming_inference.py --source webcam --prompt "person"
    python streaming_inference.py --source 0 --prompt "face"
    python streaming_inference.py --source video.mp4 --prompt "car"
    python streaming_inference.py --source rtsp://... --prompt "person"

æ³¨æ„:
    - é æœŸé€Ÿåº¦: 3-4 FPS (RTX 4060 Ti)
    - éœ€è¦ X11 é¡¯ç¤ºæ”¯æ´ (Docker ä¸­ä½¿ç”¨ ./run_docker.sh shell)
    - æŒ‰ 'q' é€€å‡º
    - æŒ‰ 's' æˆªåœ–
    - æŒ‰ 'p' æš«åœ/ç¹¼çºŒ
"""

import argparse
import os
import sys
import time
from collections import deque

# è¨­å®š OpenCV ç’°å¢ƒè®Šæ•¸ (å¿…é ˆåœ¨ import cv2 ä¹‹å‰)
os.environ['QT_QPA_PLATFORM'] = 'xcb'
os.environ['QT_X11_NO_MITSHM'] = '1'
os.environ['OPENCV_VIDEOIO_PRIORITY_MSMF'] = '0'

import cv2
import matplotlib.pyplot as plt
import numpy as np
import torch
from PIL import Image

# ç¢ºä¿å¯ä»¥ import sam3
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

import sam3
from sam3 import build_sam3_image_model
from sam3.model.sam3_image_processor import Sam3Processor

# è¨­å®šè¼¸å‡ºç›®éŒ„
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "outputs")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# è¨­å®š TF32 ä»¥æå‡ Ampere GPU æ•ˆèƒ½
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True


def parse_args():
    parser = argparse.ArgumentParser(description="SAM3 å³æ™‚ä¸²æµæ¨è«–")
    parser.add_argument(
        "--source", 
        type=str, 
        default="webcam",
        help="è¼¸å…¥ä¾†æº: 'webcam', è¦–è¨Šæª”æ¡ˆè·¯å¾‘, æˆ–æ”å½±æ©Ÿç·¨è™Ÿ (0, 1, ...)"
    )
    parser.add_argument(
        "--prompt", 
        type=str, 
        default="person",
        help="æ–‡å­—æç¤º (è¦åˆ†å‰²çš„ç‰©ä»¶)"
    )
    parser.add_argument(
        "--confidence", 
        type=float, 
        default=0.5,
        help="ä¿¡å¿ƒé–¾å€¼ (0-1)"
    )
    parser.add_argument(
        "--width",
        type=int,
        default=640,
        help="è™•ç†å¯¬åº¦ (è¼ƒå° = è¼ƒå¿«)"
    )
    parser.add_argument(
        "--height",
        type=int,
        default=480,
        help="è™•ç†é«˜åº¦ (è¼ƒå° = è¼ƒå¿«)"
    )
    parser.add_argument(
        "--no-display",
        action="store_true",
        help="ä¸é¡¯ç¤ºè¦–çª— (åƒ…å„²å­˜çµæœ)"
    )
    parser.add_argument(
        "--save-video",
        action="store_true",
        help="å„²å­˜è¼¸å‡ºè¦–è¨Š"
    )
    parser.add_argument(
        "--max-frames",
        type=int,
        default=None,
        help="æœ€å¤§è™•ç†å¹€æ•¸ (ç”¨æ–¼æ¸¬è©¦)"
    )
    parser.add_argument(
        "--show-boxes",
        action="store_true",
        default=False,
        help="é¡¯ç¤ºé‚Šç•Œæ¡† (é è¨­é—œé–‰)"
    )
    return parser.parse_args()


def overlay_masks(frame, masks, boxes, scores, alpha=0.5, show_boxes=False):
    """åœ¨å½±æ ¼ä¸Šç–ŠåŠ é®ç½©
    
    Args:
        frame: åŸå§‹å½±æ ¼
        masks: åˆ†å‰²é®ç½©
        boxes: é‚Šç•Œæ¡†
        scores: ä¿¡å¿ƒåˆ†æ•¸
        alpha: é®ç½©é€æ˜åº¦
        show_boxes: æ˜¯å¦é¡¯ç¤ºé‚Šç•Œæ¡† (True=é¡¯ç¤º, False=éš±è—)
    """
    if masks is None or len(masks) == 0:
        return frame
    
    overlay = frame.copy()
    frame_h, frame_w = frame.shape[:2]
    
    # ç”Ÿæˆé¡è‰²
    n_masks = len(masks)
    cmap = plt.colormaps.get_cmap('rainbow')
    colors = [
        tuple(int(c * 255) for c in cmap(i / max(n_masks, 1))[:3])
        for i in range(n_masks)
    ]
    
    for i, (mask, color) in enumerate(zip(masks, colors)):
        # è½‰æ›é®ç½©
        if hasattr(mask, 'cpu'):
            mask_np = mask.cpu().numpy()
        else:
            mask_np = np.array(mask)
        
        # ç¢ºä¿ mask æ˜¯ 2D
        if mask_np.ndim > 2:
            mask_np = mask_np.squeeze()
        
        # è·³éç©ºçš„ mask
        if mask_np.size == 0:
            continue
            
        # èª¿æ•´é®ç½©å¤§å°åˆ°å½±æ ¼å°ºå¯¸
        if mask_np.shape[0] != frame_h or mask_np.shape[1] != frame_w:
            mask_np = cv2.resize(
                mask_np.astype(np.uint8), 
                (frame_w, frame_h),
                interpolation=cv2.INTER_NEAREST
            )
        
        mask_bool = mask_np.astype(bool)
        
        # ç–ŠåŠ é¡è‰² (BGR)
        color_bgr = color[::-1]
        overlay[mask_bool] = (
            overlay[mask_bool] * (1 - alpha) + 
            np.array(color_bgr) * alpha
        ).astype(np.uint8)
        
        # ç¹ªè£½é‚Šç•Œæ¡† (åªåœ¨ show_boxes=True æ™‚é¡¯ç¤º)
        if show_boxes and boxes is not None and i < len(boxes):
            box = boxes[i]
            if hasattr(box, 'cpu'):
                box = box.cpu().numpy()
            x1, y1, x2, y2 = map(int, box)
            cv2.rectangle(overlay, (x1, y1), (x2, y2), color_bgr, 2)
            
            # é¡¯ç¤ºåˆ†æ•¸
            if scores is not None and i < len(scores):
                score = float(scores[i])
                label = f"{score:.2f}"
                cv2.putText(
                    overlay, label, (x1, y1 - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color_bgr, 2
                )
    
    return overlay


def main():
    args = parse_args()
    
    print("=" * 60)
    print("SAM3 å³æ™‚ä¸²æµæ¨è«–")
    print("=" * 60)
    
    # æª¢æŸ¥ GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cuda":
        print(f"âœ… ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("âš ï¸  ä½¿ç”¨ CPU (é€Ÿåº¦æœƒå¾ˆæ…¢)")
    
    # é–‹å•Ÿè¦–è¨Šä¾†æº
    if args.source == "webcam" or args.source == "0":
        cap = cv2.VideoCapture(0)
        source_name = "webcam"
    elif args.source.isdigit():
        cap = cv2.VideoCapture(int(args.source))
        source_name = f"camera_{args.source}"
    else:
        cap = cv2.VideoCapture(args.source)
        source_name = os.path.splitext(os.path.basename(args.source))[0]
    
    if not cap.isOpened():
        print(f"âŒ ç„¡æ³•é–‹å•Ÿè¦–è¨Šä¾†æº: {args.source}")
        sys.exit(1)
    
    # è¨­å®šè§£æåº¦
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, args.width)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, args.height)
    
    actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS) or 30
    
    print(f"\nğŸ“¹ è¦–è¨Šä¾†æº: {args.source}")
    print(f"   è§£æåº¦: {actual_width}x{actual_height}")
    print(f"   FPS: {fps:.1f}")
    print(f"   æç¤º: '{args.prompt}'")
    
    # æ¸¬è©¦ X11 é¡¯ç¤º
    if not args.no_display:
        try:
            # è®€å–ä¸€å¹€ä¾†æ¸¬è©¦
            ret, test_frame = cap.read()
            if ret:
                cv2.namedWindow("SAM3 Streaming", cv2.WINDOW_NORMAL)
                cv2.imshow("SAM3 Streaming", test_frame)
                cv2.waitKey(1)
                print("âœ… X11 é¡¯ç¤ºæ­£å¸¸")
            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # é‡ç½®åˆ°é–‹é ­
        except Exception as e:
            print(f"âš ï¸  X11 é¡¯ç¤ºæœ‰å•é¡Œ: {e}")
            print("   å»ºè­°ä½¿ç”¨ --no-display --save-video æ¨¡å¼")
            args.no_display = True
    
    # è¼‰å…¥æ¨¡å‹
    print("\nğŸ”§ è¼‰å…¥æ¨¡å‹...")
    sam3_root = os.path.join(os.path.dirname(sam3.__file__), "..")
    bpe_path = os.path.join(sam3_root, "assets", "bpe_simple_vocab_16e6.txt.gz")
    
    model = build_sam3_image_model(bpe_path=bpe_path, device=device)
    processor = Sam3Processor(model, confidence_threshold=args.confidence)
    
    # è¦–è¨Šå¯«å…¥å™¨
    # æ³¨æ„ï¼šä½¿ç”¨å¯¦éš›æ¨è«– FPS è€Œä¸æ˜¯æ”å½±æ©Ÿ FPSï¼Œé¿å…æ’­æ”¾åŠ é€Ÿ
    # SAM3 ç´„ 3-4 FPSï¼Œæ‰€ä»¥éŒ„è£½ç”¨é€™å€‹é€Ÿåº¦
    output_fps = 4.0  # é ä¼°çš„æ¨è«– FPS
    video_writer = None
    if args.save_video:
        output_path = os.path.join(
            OUTPUT_DIR, 
            f"{source_name}_{args.prompt.replace(' ', '_')}_stream.mp4"
        )
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(
            output_path, fourcc, output_fps,  # ä½¿ç”¨å¯¦éš›æ¨è«–é€Ÿåº¦
            (actual_width, actual_height)
        )
        print(f"ğŸ’¾ éŒ„è£½åˆ°: {output_path}")
        print(f"   è¼¸å‡º FPS: {output_fps} (ä¾å¯¦éš›æ¨è«–é€Ÿåº¦)")
    
    # FPS è¨ˆç®—
    fps_queue = deque(maxlen=30)
    frame_count = 0
    screenshot_count = 0
    paused = False
    last_result_frame = None
    
    print("\nğŸ¬ é–‹å§‹ä¸²æµæ¨è«–...")
    print("   æŒ‰ 'q' é€€å‡º")
    print("   æŒ‰ 's' æˆªåœ–")
    print("   æŒ‰ 'p' æš«åœ/ç¹¼çºŒ")
    print("   æŒ‰ 'c' æ›´æ›æç¤ºè©")
    print("-" * 60)
    
    try:
        while True:
            # è™•ç†æš«åœ
            if paused:
                if last_result_frame is not None:
                    # é¡¯ç¤ºæš«åœç‹€æ…‹
                    display_frame = last_result_frame.copy()
                    cv2.putText(
                        display_frame, "PAUSED - Press 'p' to resume", 
                        (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2
                    )
                    cv2.imshow("SAM3 Streaming", display_frame)
                
                key = cv2.waitKey(100) & 0xFF
                if key == ord('p'):
                    paused = False
                    print("\nâ–¶ï¸  ç¹¼çºŒæ¨è«–")
                elif key == ord('q'):
                    print("\nâ¹ï¸  ä½¿ç”¨è€…ä¸­æ–·")
                    break
                elif key == ord('s') and last_result_frame is not None:
                    screenshot_count += 1
                    screenshot_path = os.path.join(
                        OUTPUT_DIR,
                        f"{source_name}_screenshot_{screenshot_count:03d}.png"
                    )
                    cv2.imwrite(screenshot_path, last_result_frame)
                    print(f"\nğŸ“¸ æˆªåœ–å·²å„²å­˜: {screenshot_path}")
                continue
            
            ret, frame = cap.read()
            if not ret:
                if args.source != "webcam" and not args.source.isdigit():
                    print("\nğŸ“¹ è¦–è¨ŠçµæŸ")
                    break
                continue
            
            frame_count += 1
            if args.max_frames and frame_count > args.max_frames:
                break
            
            start_time = time.time()
            
            # è½‰æ›ç‚º PIL Image
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            
            # æ¨è«–
            with torch.autocast("cuda", dtype=torch.bfloat16):
                inference_state = processor.set_image(pil_image)
                inference_state = processor.set_text_prompt(
                    state=inference_state, 
                    prompt=args.prompt
                )
            
            # å–å¾—çµæœ
            masks = inference_state.get("masks")
            boxes = inference_state.get("boxes")
            scores = inference_state.get("scores")
            
            # ç–ŠåŠ é®ç½© (æ ¹æ“š --show-boxes æ±ºå®šæ˜¯å¦é¡¯ç¤ºé‚Šç•Œæ¡†)
            result_frame = overlay_masks(frame, masks, boxes, scores, show_boxes=args.show_boxes)
            
            # è¨ˆç®— FPS
            elapsed = time.time() - start_time
            fps_queue.append(1.0 / elapsed if elapsed > 0 else 0)
            avg_fps = sum(fps_queue) / len(fps_queue)
            
            # é¡¯ç¤ºè³‡è¨Š
            num_objects = len(masks) if masks is not None else 0
            info_text = f"FPS: {avg_fps:.1f} | Objects: {num_objects} | Prompt: {args.prompt}"
            cv2.putText(
                result_frame, info_text, (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2
            )
            
            # å„²å­˜æœ€å¾Œä¸€å¹€ (ç”¨æ–¼æš«åœæ™‚é¡¯ç¤º)
            last_result_frame = result_frame.copy()
            
            # å„²å­˜è¦–è¨Š
            if video_writer:
                video_writer.write(result_frame)
            
            # é¡¯ç¤º
            if not args.no_display:
                cv2.imshow("SAM3 Streaming", result_frame)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("\nâ¹ï¸  ä½¿ç”¨è€…ä¸­æ–·")
                    break
                elif key == ord('s'):
                    screenshot_count += 1
                    screenshot_path = os.path.join(
                        OUTPUT_DIR,
                        f"{source_name}_screenshot_{screenshot_count:03d}.png"
                    )
                    cv2.imwrite(screenshot_path, result_frame)
                    print(f"\nğŸ“¸ æˆªåœ–å·²å„²å­˜: {screenshot_path}")
                elif key == ord('p'):
                    paused = True
                    print("\nâ¸ï¸  æš«åœæ¨è«–")
            
            # é¡¯ç¤ºé€²åº¦
            if frame_count % 10 == 0:
                print(f"\r   Frame {frame_count} | FPS: {avg_fps:.1f} | Objects: {num_objects}    ", end="", flush=True)
    
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  Ctrl+C ä¸­æ–·")
    
    finally:
        cap.release()
        if video_writer:
            video_writer.release()
        if not args.no_display:
            cv2.destroyAllWindows()
    
    print("\n" + "=" * 60)
    print(f"è™•ç†å®Œæˆï¼å…± {frame_count} å¹€")
    if video_writer:
        print(f"è¦–è¨Šå·²å„²å­˜åˆ°: {output_path}")
    print("=" * 60)


if __name__ == "__main__":
    main()
