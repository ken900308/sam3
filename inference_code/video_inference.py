#!/usr/bin/env python3
"""
SAM3 è¦–è¨Šæ¨è«–è…³æœ¬
================
åŸºæ–¼å®˜æ–¹ examples/sam3_video_predictor_example.ipynb ä¿®æ”¹

ä½¿ç”¨æ–¹æ³•:
    python video_inference.py --video <è¦–è¨Šè·¯å¾‘> --prompt "è¦è¿½è¹¤çš„ç‰©ä»¶"

ç¯„ä¾‹:
    python video_inference.py --video ../assets/videos/0001 --prompt "person"
    python video_inference.py --video my_video.mp4 --prompt "car"

è¼¸å…¥æ ¼å¼:
    - MP4 è¦–è¨Šæª”æ¡ˆ
    - æˆ– JPEG å½±æ ¼è³‡æ–™å¤¾ (æª”åæ ¼å¼: 00000.jpg, 00001.jpg, ...)
"""

import argparse
import glob
import os
import sys
import time

import cv2
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import numpy as np
import torch
from PIL import Image

# ç¢ºä¿å¯ä»¥ import sam3
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

import sam3
from sam3.model_builder import build_sam3_video_predictor
from sam3.visualization_utils import (
    load_frame,
    prepare_masks_for_visualization,
    visualize_formatted_frame_output,
)

# è¨­å®šè¼¸å‡ºç›®éŒ„
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "outputs")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# è¨­å®š TF32 ä»¥æå‡ Ampere GPU æ•ˆèƒ½
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True


def parse_args():
    parser = argparse.ArgumentParser(description="SAM3 è¦–è¨Šæ¨è«–")
    parser.add_argument(
        "--video", 
        type=str, 
        default=None,
        help="è¼¸å…¥è¦–è¨Šè·¯å¾‘ (MP4 æª”æ¡ˆæˆ– JPEG è³‡æ–™å¤¾)"
    )
    parser.add_argument(
        "--prompt", 
        type=str, 
        default="person",
        help="æ–‡å­—æç¤º (è¦è¿½è¹¤çš„ç‰©ä»¶)"
    )
    parser.add_argument(
        "--output", 
        type=str, 
        default=None,
        help="è¼¸å‡ºè¦–è¨Šè·¯å¾‘ (é è¨­è‡ªå‹•ç”Ÿæˆ)"
    )
    parser.add_argument(
        "--save-frames",
        action="store_true",
        help="æ˜¯å¦å„²å­˜æ¯ä¸€å¹€ç‚ºåœ–ç‰‡"
    )
    parser.add_argument(
        "--vis-stride",
        type=int,
        default=30,
        help="è¦–è¦ºåŒ–é–“éš” (æ¯ N å¹€å„²å­˜ä¸€å¼µ)"
    )
    parser.add_argument(
        "--gpus",
        type=str,
        default=None,
        help="ä½¿ç”¨çš„ GPU (ä¾‹å¦‚: '0' æˆ– '0,1')"
    )
    return parser.parse_args()


def load_video_frames(video_path):
    """è¼‰å…¥è¦–è¨Šå¹€ (æ”¯æ´ MP4 å’Œ JPEG è³‡æ–™å¤¾)"""
    if isinstance(video_path, str) and video_path.endswith(".mp4"):
        cap = cv2.VideoCapture(video_path)
        frames = []
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        cap.release()
        return frames
    else:
        frames = glob.glob(os.path.join(video_path, "*.jpg"))
        try:
            frames.sort(key=lambda p: int(os.path.splitext(os.path.basename(p))[0]))
        except ValueError:
            frames.sort()
        return frames


def propagate_in_video(predictor, session_id):
    """åœ¨è¦–è¨Šä¸­å‚³æ’­åˆ†å‰²çµæœ"""
    outputs_per_frame = {}
    for response in predictor.handle_stream_request(
        request=dict(
            type="propagate_in_video",
            session_id=session_id,
        )
    ):
        outputs_per_frame[response["frame_index"]] = response["outputs"]
    return outputs_per_frame


def save_video_with_masks(video_frames, outputs_per_frame, output_path, fps=24):
    """å°‡åˆ†å‰²çµæœå„²å­˜ç‚ºè¦–è¨Š"""
    # æº–å‚™é®ç½© (prepare_masks_for_visualization æœƒä¿®æ”¹åŸå§‹ dict)
    # æ ¼å¼: {frame_idx: {obj_id: binary_mask, ...}}
    formatted_outputs = prepare_masks_for_visualization(outputs_per_frame.copy())
    
    # å–å¾—è¦–è¨Šå°ºå¯¸
    if isinstance(video_frames[0], str):
        sample = cv2.imread(video_frames[0])
        height, width = sample.shape[:2]
    else:
        height, width = video_frames[0].shape[:2]
    
    # å»ºç«‹è¦–è¨Šå¯«å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    # å¯«å…¥æ¯ä¸€å¹€
    for frame_idx in range(len(video_frames)):
        # è¼‰å…¥åŸå§‹å¹€
        if isinstance(video_frames[frame_idx], str):
            frame = cv2.imread(video_frames[frame_idx])
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        else:
            frame = video_frames[frame_idx]
        
        # ç–ŠåŠ é®ç½©
        if frame_idx in formatted_outputs:
            masks_dict = formatted_outputs[frame_idx]
            # masks_dict æ ¼å¼: {obj_id: binary_mask (np.ndarray)}
            for obj_id, mask in masks_dict.items():
                if mask is not None and mask.any():
                    # ç”Ÿæˆé¡è‰² (æ ¹æ“š obj_id)
                    color = plt.cm.rainbow(obj_id / 10)[:3]
                    color = np.array(color) * 255
                    
                    # ç–ŠåŠ é®ç½©
                    mask_bool = mask.astype(bool)
                    overlay = frame.copy()
                    overlay[mask_bool] = overlay[mask_bool] * 0.5 + color * 0.5
                    frame = overlay.astype(np.uint8)
        
        # å¯«å…¥å¹€
        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        writer.write(frame_bgr)
    
    writer.release()
    print(f"âœ… è¦–è¨Šå·²å„²å­˜åˆ°: {output_path}")


def main():
    args = parse_args()
    
    print("=" * 60)
    print("SAM3 è¦–è¨Šæ¨è«–")
    print("=" * 60)
    
    # è¨­å®š GPU
    if args.gpus is not None:
        gpus_to_use = [int(g) for g in args.gpus.split(",")]
    else:
        gpus_to_use = list(range(torch.cuda.device_count()))
    
    if len(gpus_to_use) > 0 and torch.cuda.is_available():
        print(f"âœ… ä½¿ç”¨ GPU: {gpus_to_use}")
    else:
        print("âš ï¸  ç„¡å¯ç”¨ GPU")
        sys.exit(1)
    
    # è¨­å®šè¦–è¨Šè·¯å¾‘
    sam3_root = os.path.join(os.path.dirname(sam3.__file__), "..")
    if args.video is None:
        video_path = os.path.join(sam3_root, "assets", "videos", "0001")
    else:
        video_path = args.video
        if not os.path.isabs(video_path):
            video_path = os.path.abspath(video_path)
    
    if not os.path.exists(video_path):
        print(f"âŒ æ‰¾ä¸åˆ°è¦–è¨Š: {video_path}")
        sys.exit(1)
    
    print(f"\nğŸ“¹ è¼‰å…¥è¦–è¨Š: {video_path}")
    video_frames = load_video_frames(video_path)
    print(f"   å…± {len(video_frames)} å¹€")
    
    # å»ºç«‹ predictor
    print("\nğŸ”§ è¼‰å…¥æ¨¡å‹...")
    predictor = build_sam3_video_predictor(gpus_to_use=gpus_to_use)
    
    # é–‹å§‹ session
    print("ğŸ¬ é–‹å§‹æ¨è«– session...")
    start_time = time.time()
    
    response = predictor.handle_request(
        request=dict(
            type="start_session",
            resource_path=video_path,
        )
    )
    session_id = response["session_id"]
    
    # æ·»åŠ æ–‡å­—æç¤º
    print(f"ğŸ” ä½¿ç”¨æ–‡å­—æç¤º: '{args.prompt}'")
    response = predictor.handle_request(
        request=dict(
            type="add_prompt",
            session_id=session_id,
            frame_index=0,
            text=args.prompt,
        )
    )
    
    initial_output = response["outputs"]
    num_objects = len(initial_output.get("obj_ids", []))
    print(f"   åœ¨ç¬¬ 0 å¹€æ‰¾åˆ° {num_objects} å€‹ç‰©ä»¶")
    
    # å‚³æ’­åˆ°æ•´å€‹è¦–è¨Š
    print("ğŸ“¤ å‚³æ’­åˆ°æ•´å€‹è¦–è¨Š...")
    outputs_per_frame = propagate_in_video(predictor, session_id)
    
    elapsed = time.time() - start_time
    fps = len(video_frames) / elapsed
    print(f"\nâ±ï¸  ç¸½è€—æ™‚: {elapsed:.2f} ç§’ ({fps:.2f} FPS)")
    
    # å„²å­˜çµæœ
    if args.output is None:
        base_name = os.path.basename(video_path.rstrip("/"))
        prompt_safe = args.prompt.replace(" ", "_")[:20]
        output_path = os.path.join(OUTPUT_DIR, f"{base_name}_{prompt_safe}_result.mp4")
    else:
        output_path = args.output
    
    print(f"\nğŸ’¾ å„²å­˜çµæœ...")
    save_video_with_masks(video_frames, outputs_per_frame, output_path)
    
    # å„²å­˜é—œéµå¹€
    if args.save_frames:
        frames_dir = os.path.join(OUTPUT_DIR, f"{base_name}_{prompt_safe}_frames")
        os.makedirs(frames_dir, exist_ok=True)
        
        formatted_outputs = prepare_masks_for_visualization(outputs_per_frame)
        for frame_idx in range(0, len(video_frames), args.vis_stride):
            plt.figure(figsize=(10, 8))
            visualize_formatted_frame_output(
                frame_idx,
                video_frames,
                outputs_list=[formatted_outputs],
                titles=[f"Frame {frame_idx}"],
                figsize=(10, 8),
            )
            frame_path = os.path.join(frames_dir, f"frame_{frame_idx:05d}.png")
            plt.savefig(frame_path, dpi=100, bbox_inches='tight')
            plt.close()
        
        print(f"âœ… é—œéµå¹€å·²å„²å­˜åˆ°: {frames_dir}")
    
    # é—œé–‰ session
    predictor.handle_request(
        request=dict(
            type="close_session",
            session_id=session_id,
        )
    )
    predictor.shutdown()
    
    print("\n" + "=" * 60)
    print("æ¨è«–å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
