#!/usr/bin/env python3
"""
SAM3 Docker ç’°å¢ƒæ¸¬è©¦è…³æœ¬
æ¸¬è©¦ CUDAã€PyTorchã€ROS å’Œ SAM3 æ˜¯å¦æ­£ç¢ºå®‰è£
"""

import sys
import subprocess


def print_section(title):
    """æ‰“å°ç« ç¯€æ¨™é¡Œ"""
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def check_python():
    """æª¢æŸ¥ Python ç‰ˆæœ¬"""
    print_section("Python ç‰ˆæœ¬æª¢æŸ¥")
    print(f"Python ç‰ˆæœ¬: {sys.version}")
    print(f"Python å¯åŸ·è¡Œæ–‡ä»¶: {sys.executable}")
    
    required_version = (3, 12)
    current_version = sys.version_info[:2]
    
    if current_version >= required_version:
        print(f"âœ… Python {current_version[0]}.{current_version[1]} >= {required_version[0]}.{required_version[1]}")
        return True
    else:
        print(f"âŒ Python {current_version[0]}.{current_version[1]} < {required_version[0]}.{required_version[1]}")
        return False


def check_pytorch():
    """æª¢æŸ¥ PyTorch å’Œ CUDA"""
    print_section("PyTorch å’Œ CUDA æª¢æŸ¥")
    
    try:
        import torch
        print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            print(f"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
            print(f"GPU æ•¸é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
                props = torch.cuda.get_device_properties(i)
                print(f"  - ç¸½è¨˜æ†¶é«”: {props.total_memory / 1024**3:.2f} GB")
                print(f"  - è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}")
            
            # ç°¡å–®çš„ CUDA æ¸¬è©¦
            print("\næ¸¬è©¦ CUDA é‹ç®—...")
            x = torch.randn(1000, 1000).cuda()
            y = torch.randn(1000, 1000).cuda()
            z = torch.matmul(x, y)
            print(f"âœ… CUDA çŸ©é™£é‹ç®—æˆåŠŸ: {z.shape}")
            
            return True
        else:
            print("âŒ CUDA ä¸å¯ç”¨")
            return False
            
    except Exception as e:
        print(f"âŒ PyTorch æª¢æŸ¥å¤±æ•—: {e}")
        return False


def check_ros():
    """æª¢æŸ¥ ROS ç’°å¢ƒ"""
    print_section("ROS ç’°å¢ƒæª¢æŸ¥")
    
    import os
    
    ros_distro = os.environ.get('ROS_DISTRO')
    ros_version = os.environ.get('ROS_VERSION')
    
    print(f"ROS_DISTRO: {ros_distro}")
    print(f"ROS_VERSION: {ros_version}")
    
    if ros_distro and ros_version:
        # å˜—è©¦é‹è¡Œ ROS å‘½ä»¤
        try:
            result = subprocess.run(
                ['ros2', '--version'],
                capture_output=True,
                text=True,
                timeout=5
            )
            print(f"ROS2 ç‰ˆæœ¬: {result.stdout.strip()}")
            print(f"âœ… ROS Humble ç’°å¢ƒæ­£å¸¸")
            return True
        except Exception as e:
            print(f"âŒ ROS å‘½ä»¤åŸ·è¡Œå¤±æ•—: {e}")
            return False
    else:
        print("âŒ ROS ç’°å¢ƒè®Šæ•¸æœªè¨­ç½®")
        return False


def check_dependencies():
    """æª¢æŸ¥é—œéµä¾è³´"""
    print_section("ä¾è³´å¥—ä»¶æª¢æŸ¥")
    
    packages = [
        'numpy',
        'torch',
        'torchvision',
        'timm',
        'transformers',
        'huggingface_hub',
        'opencv-python',
        'matplotlib',
        'pillow',
        'tqdm',
    ]
    
    all_ok = True
    for package in packages:
        try:
            # è™•ç†ç‰¹æ®Šçš„å¥—ä»¶åç¨±
            import_name = package
            if package == 'opencv-python':
                import_name = 'cv2'
            elif package == 'pillow':
                import_name = 'PIL'
            
            module = __import__(import_name.replace('-', '_'))
            version = getattr(module, '__version__', 'unknown')
            print(f"âœ… {package}: {version}")
        except ImportError:
            print(f"âŒ {package}: æœªå®‰è£")
            all_ok = False
    
    return all_ok


def check_sam3():
    """æª¢æŸ¥ SAM3 å®‰è£"""
    print_section("SAM3 å®‰è£æª¢æŸ¥")
    
    try:
        # å˜—è©¦å°å…¥ SAM3
        from sam3 import __version__
        print(f"SAM3 ç‰ˆæœ¬: {__version__}")
        
        # æ¸¬è©¦ä¸»è¦çµ„ä»¶
        print("\næ¸¬è©¦ SAM3 çµ„ä»¶å°å…¥...")
        
        components = [
            ('sam3.model_builder', 'build_sam3_image_model'),
            ('sam3.model.sam3_image', 'Sam3Image'),
            ('sam3.model.sam3_image_processor', 'Sam3Processor'),
        ]
        
        all_ok = True
        for module_name, component_name in components:
            try:
                module = __import__(module_name, fromlist=[component_name])
                getattr(module, component_name)
                print(f"âœ… {module_name}.{component_name}")
            except Exception as e:
                print(f"âŒ {module_name}.{component_name}: {e}")
                all_ok = False
        
        if all_ok:
            print("\nâœ… SAM3 æ‰€æœ‰çµ„ä»¶å°å…¥æˆåŠŸ")
            return True
        else:
            print("\nâŒ éƒ¨åˆ† SAM3 çµ„ä»¶å°å…¥å¤±æ•—")
            return False
            
    except ImportError as e:
        print(f"âŒ SAM3 æœªå®‰è£æˆ–å°å…¥å¤±æ•—: {e}")
        print("\nå»ºè­°åŸ·è¡Œ: pip install -e /workspace/sam3")
        return False


def check_gpu_memory():
    """æª¢æŸ¥ GPU è¨˜æ†¶é«”"""
    print_section("GPU è¨˜æ†¶é«”æª¢æŸ¥")
    
    try:
        import torch
        
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                print(f"\nGPU {i}: {torch.cuda.get_device_name(i)}")
                
                # è¨˜æ†¶é«”è³‡è¨Š
                total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                reserved = torch.cuda.memory_reserved(i) / 1024**3
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                free = total - reserved
                
                print(f"  ç¸½è¨˜æ†¶é«”: {total:.2f} GB")
                print(f"  å·²ä¿ç•™: {reserved:.2f} GB")
                print(f"  å·²åˆ†é…: {allocated:.2f} GB")
                print(f"  å¯ç”¨: {free:.2f} GB")
                
                if total < 8:
                    print(f"  âš ï¸  è¨˜æ†¶é«”å¯èƒ½ä¸è¶³ï¼Œå»ºè­° 16GB+")
            
            return True
        else:
            print("âŒ ç„¡å¯ç”¨ GPU")
            return False
            
    except Exception as e:
        print(f"âŒ GPU è¨˜æ†¶é«”æª¢æŸ¥å¤±æ•—: {e}")
        return False


def print_summary(results):
    """æ‰“å°æ¸¬è©¦æ‘˜è¦"""
    print_section("æ¸¬è©¦æ‘˜è¦")
    
    total = len(results)
    passed = sum(results.values())
    
    for test_name, result in results.items():
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        print(f"{test_name:.<40} {status}")
    
    print(f"\nç¸½è¨ˆ: {passed}/{total} é …æ¸¬è©¦é€šé")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼SAM3 ç’°å¢ƒå·²æº–å‚™å°±ç·’ã€‚")
        return True
    else:
        print("\nâš ï¸  éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ä¸Šè¿°éŒ¯èª¤ä¿¡æ¯ã€‚")
        return False


def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 60)
    print("  SAM3 Docker ç’°å¢ƒæ¸¬è©¦")
    print("=" * 60)
    
    # é‹è¡Œæ‰€æœ‰æ¸¬è©¦
    results = {
        'Python ç‰ˆæœ¬': check_python(),
        'PyTorch å’Œ CUDA': check_pytorch(),
        'ROS ç’°å¢ƒ': check_ros(),
        'ä¾è³´å¥—ä»¶': check_dependencies(),
        'GPU è¨˜æ†¶é«”': check_gpu_memory(),
        'SAM3 å®‰è£': check_sam3(),
    }
    
    # æ‰“å°æ‘˜è¦
    success = print_summary(results)
    
    # è¿”å›é€€å‡ºç¢¼
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
